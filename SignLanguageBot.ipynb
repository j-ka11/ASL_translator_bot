{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FSym1Ghnu68X",
    "outputId": "8c33ac08-24d4-4779-ffc0-d4c61e810705"
   },
   "outputs": [],
   "source": [
    "!pip install ffmpeg-python\n",
    "!pip install SpeechRecognition\n",
    "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n",
    "!pip install PyAudio\n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "from IPython.display import Image\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import speech_recognition as sr\n",
    "from IPython.display import Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "from scipy.io.wavfile import read as wav_read\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "import ffmpeg\n",
    "import requests, zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_IaGvjRolPv",
    "outputId": "c79a7497-a748-4e74-d089-963eb00d79c5"
   },
   "outputs": [],
   "source": [
    "nn = tf.keras.models.load_model('model/')\n",
    "nn.load_weights('weights/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAS7l_L5u-9I"
   },
   "outputs": [],
   "source": [
    "# IMAGE CAPTURING CODE\n",
    "def predictLetter(imageFileName):\n",
    "    trainImage = PIL.Image.open('/content/'+imageFileName)\n",
    "    trainImagePred = []\n",
    "    trainImagePred.append(np.asarray(trainImage))\n",
    "    trainImagePred = np.asarray(trainImagePred) / 255\n",
    "    trainPreds = nn.predict(trainImagePred)\n",
    "    letterPred = chr(np.argmax(trainPreds[0]) + 65)\n",
    "    return letterPred\n",
    "\n",
    "#Video capture code provided by Google Colab\n",
    "def take_photo(filename='photo.jpg', quality=0.8):\n",
    "  js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      div.innerHTML = \"Please center hand in the frame and try to maintain a clear background. Click on the console and then press any key to take a picture.\"\n",
    "\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      // Wait for Capture to be clicked.\n",
    "      await new Promise((resolve) => {window.addEventListener('keypress', resolve)});\n",
    "\n",
    "      const canvas = document.createElement('canvas');\n",
    "      canvas.width = video.videoWidth;\n",
    "      canvas.height = video.videoHeight;\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "      stream.getVideoTracks()[0].stop();\n",
    "      div.remove();\n",
    "      return canvas.toDataURL('image/jpeg', quality);\n",
    "    }\n",
    "    ''')\n",
    "  display(js)\n",
    "  data = eval_js('takePhoto({})'.format(quality))\n",
    "  binary = b64decode(data.split(',')[1])\n",
    "  with open(filename, 'wb') as f:\n",
    "    f.write(binary)\n",
    "  return filename\n",
    "\n",
    "def doCamera():\n",
    "  phrase = \"\"\n",
    "  while True:\n",
    "    try:\n",
    "      filename = take_photo()\n",
    "      img = cv2.imread(filename, 1)\n",
    "      x, y, z = img.shape\n",
    "      y_top = int(y/2)-200\n",
    "      y_bot = int(y/2)+200\n",
    "      x_left = int(x/2)-200\n",
    "      x_right = int(x/2)+200\n",
    "      img = img[x_left:x_right, y_top:y_bot]\n",
    "      img = cv2.resize(img, (200, 200))\n",
    "      cv2.imwrite(filename, img)\n",
    "\n",
    "      # Show the image which was just taken.\n",
    "      display(Image(filename))\n",
    "      # loop = asyncio.get_event_loop()\n",
    "      # loop.run_until_complete(predict(filename))\n",
    "      letter = predictLetter(filename)\n",
    "      if letter == '[':\n",
    "        break\n",
    "      elif letter == ']':\n",
    "        letter = \" \"\n",
    "      print(letter)\n",
    "      phrase += letter\n",
    "      \n",
    "    except Exception as err:\n",
    "      # Errors will be thrown if the user does not have a webcam or if they do not\n",
    "      # grant the page permission to access it.\n",
    "      print(str(err))\n",
    "  print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSEww_jjwPnh"
   },
   "outputs": [],
   "source": [
    "# SPEECH TO TEXT CODE\n",
    "#code snippets from https://colab.research.google.com/drive/1Z6VIRZ_sX314hyev3Gm5gBqvm1wQVo-a#scrollTo=xuKJ4wBU6gxx \n",
    "#and https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be#file-record-py\n",
    "\n",
    "def record(sec=3):\n",
    "  RECORD = \"\"\"\n",
    "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
    "const b2text = blob => new Promise(resolve => {\n",
    "  const reader = new FileReader()\n",
    "  reader.onloadend = e => resolve(e.srcElement.result)\n",
    "  reader.readAsDataURL(blob)\n",
    "})\n",
    "var record = time => new Promise(async resolve => {\n",
    "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
    "  recorder = new MediaRecorder(stream)\n",
    "  chunks = []\n",
    "  recorder.ondataavailable = e => chunks.push(e.data)\n",
    "  recorder.start()\n",
    "  await sleep(time)\n",
    "  recorder.onstop = async ()=>{\n",
    "    blob = new Blob(chunks)\n",
    "    text = await b2text(blob)\n",
    "    resolve(text)\n",
    "  }\n",
    "  recorder.stop()\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "  display(Javascript(RECORD))\n",
    "  s = eval_js('record(%d)' % (sec*1000))\n",
    "  b = b64decode(s.split(',')[1])\n",
    "\n",
    "  process = (ffmpeg\n",
    "    .input('pipe:0')\n",
    "    .output('pipe:1', format='wav')\n",
    "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
    "  )\n",
    "  output, err = process.communicate(input=b)\n",
    "  riff_chunk_size = len(output) - 8\n",
    "  # Break up the chunk size into four bytes, held in b.\n",
    "  q = riff_chunk_size\n",
    "  b = []\n",
    "  for i in range(4):\n",
    "      q, r = divmod(q, 256)\n",
    "      b.append(r)\n",
    "\n",
    "  riff = output[:4] + bytes(b) + output[8:]\n",
    "\n",
    "  sr, audio = wav_read(io.BytesIO(riff))\n",
    "  wav_write('audio.wav', sr, audio)\n",
    "  \n",
    "def speechToText():\n",
    "  #import library\n",
    "  # Initialize recognizer class (for recognizing the speech)\n",
    "  r = sr.Recognizer()\n",
    "  # Reading Microphone as source\n",
    "  # listening the speech and store in audio_text variable\n",
    "  print(\"Talk\")\n",
    "  a = record()\n",
    "  audio = sr.AudioFile('audio.wav')\n",
    "\n",
    "  with audio as source:\n",
    "    audio_text = r.record(source)\n",
    "\n",
    "    print(\"Time over, thanks\")\n",
    "  # recoginize_() method will throw a request error if the API is unreachable, hence using exception handling\n",
    "  #using google speech recognition\n",
    "  speech_string = r.recognize_google(audio_text)\n",
    "  #print(\"Text: \"+speech_string)\n",
    "  return speech_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ivASOtoQhQmH",
    "outputId": "1629ff75-dbaf-4e51-c7d2-6aa884e7c1da"
   },
   "outputs": [],
   "source": [
    "  # MAIN WHILE LOOP\n",
    "run = True\n",
    "while run:\n",
    "  command = input(\"Enter 1 to do Sign Language, 2 to do Speech to Text and 3 to quit: \")\n",
    "  if command == \"1\":\n",
    "    doCamera() \n",
    "  elif command == \"2\":\n",
    "    alreadyRan = False\n",
    "    while alreadyRan == False:\n",
    "      print('This is what you said: '+ speechToText())\n",
    "      alreadyRan = True\n",
    "  elif command == \"3\":\n",
    "    print(\"QUITTING\")\n",
    "    run = False\n",
    "  else:\n",
    "    print(\"NOT A VALID COMMAND\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SignLanguageBot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
